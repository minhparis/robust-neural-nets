Context:
Machine learning recent work has shown that even advanced deep neural networks are vulnerable to a class of malicious inputs known as conflicting examples. These examples are non-random inputs that are almost indistinguishable from natural data and yet are misclassified.

Adversarial Examples:
By modifying some bits of the data, our systems could predict that it is a benign file and would not block it, posing a security risk to the computer.

Three approaches will be discussed here:

- Fast Gradient Sign Method
- Projected Gradient Descent
- Black Box attacks
